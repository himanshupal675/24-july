{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca6a2737-4685-4364-98fe-c9c8d9037d2e",
   "metadata": {},
   "source": [
    "## Answer1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8c806d-87e4-44c5-9f68-f1b86c8a444a",
   "metadata": {},
   "source": [
    "R-Squared (R² or the coefficient of determination) is a statistical measure in a regression model that determines the proportion of variance in the dependent variable that can be explained by the independent variable. In other words, r-squared shows how well the data fit the regression model (the goodness of fit).\n",
    "\n",
    "R^2 = 1 − sum squared regression (SSR) total sum of squares (SST) , = 1 − ∑ ( y i − y i ^ ) 2 ∑ ( y i − y ¯ ) 2 . The sum squared regression is the sum of the residuals squared, and the total sum of squares is the sum of the distance the data is away from the mean all squared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ac6605-7230-4a92-9e93-ce973d41cbf1",
   "metadata": {},
   "source": [
    "## Answer 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05cb1c6-7e15-47dd-a9da-aecd1cf1e6b5",
   "metadata": {},
   "source": [
    "Adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model. The adjusted R-squared increases when the new term improves the model more than would be expected by chance. It decreases when a predictor improves the model by less than expected.\n",
    "\n",
    "Adjusted R-squared and predicted R-square help you resist the urge to add too many independent variables to your model. Adjusted R-square compares models with different numbers of variables. Predicted R-square can guard against models that are too complicated.\n",
    "\n",
    "The predicted R-squared, unlike the adjusted R-squared, is used to indicate how well a regression model predicts responses for new observations. So where the adjusted R-squared can provide an accurate model that fits the current data, the predicted R-squared determines how likely it is that this model will be accurate for future data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5377a1-38e8-4518-ba45-fd43b2fa04da",
   "metadata": {},
   "source": [
    "## Answer 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e095c0-d7d7-4cca-9cc6-02f4cff9ac6c",
   "metadata": {},
   "source": [
    "Using adjusted R-squared over R-squared may be favored because of its ability to make a more accurate view of the correlation between one variable and another. Adjusted R-squared does this by taking into account how many independent variables are added to a particular model against which the stock index is measured."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fe6140-04a6-47ee-8561-be8f9a1dd9d5",
   "metadata": {},
   "source": [
    "## Answer 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa37c639-e1ff-476a-a904-d36dd9d19bac",
   "metadata": {},
   "source": [
    " The MSE, MAE, RMSE, and R-Squared metrics are mainly used to evaluate the prediction error rates and model performance in regression analysis.\n",
    "MAE (Mean absolute error) represents the difference between the original and predicted values extracted by averaged the absolute difference over the data set.\n",
    "MSE (Mean Squared Error) represents the difference between the original and predicted values extracted by squared the average difference over the data set.\n",
    "RMSE (Root Mean Squared Error) is the error rate by the square root of MSE.\n",
    "R-squared (Coefficient of determination) represents the coefficient of how well the values fit compared to the original values. The value from 0 to 1 interpreted as percentages. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c889fd77-f75e-4fe1-a7c8-f7d21bd96802",
   "metadata": {},
   "source": [
    "## Answer 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b404a3-42ef-46a4-86bc-8fd74416faeb",
   "metadata": {},
   "source": [
    "RMSE:==>\n",
    "Like MSE, RMSE is dependent on the scale of the data. It increases in magnitude if the scale of the error increases. One major drawback of RMSE is its sensitivity to outliers and the outliers have to be removed for it to function properly. RMSE increases with an increase in the size of the test sample.\n",
    "\n",
    "MSE:====>\n",
    "Advantage: The MSE is great for ensuring that our trained model has no outlier predictions with huge errors, since the MSE puts larger weight on theses errors due to the squaring part of the function.\n",
    "\n",
    "Disadvantage: If our model makes a single very bad prediction, the squaring part of the function magnifies the error. Yet in many practical cases we don’t care much about these outliers and are aiming for more of a well-rounded model that performs good enough on the majority.\n",
    "\n",
    "MAE:===>\n",
    "Advantage: The beauty of the MAE is that its advantage directly covers the MSE disadvantage. Since we are taking the absolute value, all of the errors will be weighted on the same linear scale. Thus, unlike the MSE, we won’t be putting too much weight on our outliers and our loss function provides a generic and even measure of how well our model is performing.\n",
    "\n",
    "Disadvantage: If we do in fact care about the outlier predictions of our model, then the MAE won’t be as effective. The large errors coming from the outliers end up being weighted the exact same as lower errors. This might results in our model being great most of the time, but making a few very poor predictions every so-often."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0decba7-4b9a-4f3d-a309-d88e6cc2f00e",
   "metadata": {},
   "source": [
    "## Answer 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480f1a02-3c35-4b77-8237-d4d7210c5ea8",
   "metadata": {},
   "source": [
    "Lasso regression is a regularization technique. It is used over regression methods for a more accurate prediction. This model uses shrinkage. Shrinkage is where data values are shrunk towards a central point as the mean. The lasso procedure encourages simple, sparse models (i.e. models with fewer parameters).\n",
    "\n",
    "Similar to the lasso regression, ridge regression puts a similar constraint on the coefficients by introducing a penalty factor. However, while lasso regression takes the magnitude of the coefficients, ridge regression takes the square. Ridge regression is also referred to as L2 Regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4c1422-0c10-460b-953b-4f3b83756889",
   "metadata": {},
   "source": [
    "## Answer 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da6e478-800c-4b15-b1b0-837162a6d05e",
   "metadata": {},
   "source": [
    "Regularization works by adding a penalty or complexity term to the complex model. Let's consider the simple linear regression equation:\n",
    "\n",
    "y= β0+β1x1+β2x2+β3x3+⋯+βnxn +b\n",
    "In the above equation, Y represents the value to be predicted\n",
    "\n",
    "X1, X2, …Xn are the features for Y.\n",
    "\n",
    "β0,β1,…..βn are the weights or magnitude attached to the features, respectively. Here represents the bias of the model, and b represents the intercept.\n",
    "\n",
    "Linear regression models try to optimize the β0 and b to minimize the cost function. The equation for the cost function for the linear model is given below:\n",
    "\n",
    "Regularization in Machine Learning\n",
    "Now, we will add a loss function and optimize parameter to make the model that can predict the accurate value of Y. The loss function for the linear regression is called as RSS or Residual sum of squares.\n",
    "\n",
    "Techniques of Regularization\n",
    "There are mainly two types of regularization techniques, which are given below:\n",
    "\n",
    "Ridge Regression\n",
    "Lasso Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b335f561-1e44-41d1-9ca7-08785aa397af",
   "metadata": {},
   "source": [
    "## Answer 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7db7ea-30a2-4744-aa76-b54114411173",
   "metadata": {},
   "source": [
    "It is assumed that the cause and effect relationship between the variables remains unchanged. This assumption may not always hold good and hence estimation of the values of a variable made on the basis of the regression equation may lead to erroneous and misleading results.\n",
    "The functional relationship that is established between any two or more variables on the basis of some limited data may not hold good if more and more data are taken into consideration. For example, in case of the Law of Return, the law of diminishing return may come to play, if too much of inputs are used with ca view to increasing the volume of output.\n",
    "It involves very lengthy and complicated procedure of calculations and analysis.\n",
    "It cannot be used in case of qualitative phenomenon viz. honesty, crime etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c5b5c1-49b5-4a64-be47-55c9a0c8fc1a",
   "metadata": {},
   "source": [
    "## Answer 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72630fbf-5f31-4b23-9145-df5a14c26525",
   "metadata": {},
   "source": [
    "Choosing the better model between Model A and Model B depends on the specific goals and requirements of the problem at hand. However, based solely on the given evaluation metrics, we can make some observations.\n",
    "\n",
    "Model A has an RMSE (Root Mean Squared Error) of 10, which indicates that on average, the model's predictions are off by 10 units. RMSE is sensitive to outliers, as it squares the errors, and is often used when large errors are particularly undesirable.\n",
    "\n",
    "On the other hand, Model B has an MAE (Mean Absolute Error) of 8, which indicates that, on average, the model's predictions are off by 8 units. MAE is less sensitive to outliers than RMSE, as it takes the absolute value of errors.\n",
    "\n",
    "Therefore, if the problem at hand is particularly sensitive to large errors, we might prefer Model A since it has a smaller MAE. On the other hand, if we are less concerned about outliers and want a model with lower overall error, we might choose Model B since it has a smaller RMSE.\n",
    "\n",
    "It's worth noting that both evaluation metrics have their limitations. RMSE is sensitive to outliers, and its value can be influenced by extreme values in the data. Additionally, it can be challenging to interpret the magnitude of RMSE since it is in the same unit as the dependent variable. MAE is less sensitive to outliers but does not penalize large errors as much as RMSE does. Therefore, it's essential to consider both metrics and other evaluation metrics when comparing models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5330a1b1-df5d-4331-bb6d-c79897490728",
   "metadata": {},
   "source": [
    "## Answer 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21f5c18-21c4-4144-a386-6260e9fbf837",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Choosing the better model between Model A and Model B depends on the specific goals and requirements of the problem at hand. However, based solely on the given information, we can make some observations.\n",
    "\n",
    "Ridge regularization (L2 regularization) penalizes the sum of the squared values of the model's coefficients, while Lasso regularization (L1 regularization) penalizes the sum of the absolute values of the coefficients.\n",
    "\n",
    "The choice between these regularization methods depends on the problem's requirements and the nature of the data. If the data contains many features that are potentially relevant, we might prefer Ridge regularization, which shrinks the coefficients towards zero without setting them exactly to zero, allowing all features to contribute to the model. On the other hand, if we believe that only a subset of features is relevant, we might prefer Lasso regularization, which sets some coefficients to exactly zero and performs feature selection.\n",
    "\n",
    "Model A uses Ridge regularization with a regularization parameter of 0.1. A smaller regularization parameter leads to less regularization, allowing the model to fit the data more closely, but increasing the risk of overfitting. Model B uses Lasso regularization with a larger regularization parameter of 0.5, which means more regularization and a greater tendency towards sparsity. \n",
    "\n",
    "Therefore, if we are more interested in selecting a subset of relevant features, we might choose Model B since Lasso regularization can perform feature selection. On the other hand, if we are more concerned about the model's generalization performance, we might choose Model A since a smaller regularization parameter may lead to better fit on the training set and potentially better generalization performance.\n",
    "\n",
    "It's important to note that both regularization methods have their limitations. Ridge regularization tends to perform better when the coefficients have similar magnitudes, while Lasso regularization tends to perform better when there are only a few significant features. Additionally, the choice of regularization parameter depends on the specific problem and may require tuning via cross-validation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
